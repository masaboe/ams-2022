{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction BOW\n",
    "===\n",
    "\n",
    "**Modul untuk Pembelajaran Analitika Media Sosial**\n",
    "\n",
    "**Author:** *Dr. Eng. Farrikh Alzami, M.Kom*, *Abu Salam, M.Kom*\n",
    "\n",
    "Program Studi Sistem Informasi S1\n",
    "\n",
    "***Universitas Dian Nuswantoro***\n",
    "\n",
    "\n",
    "TEORI\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Bag of Words\n",
    "\n",
    "## apa itu Bag of Words\n",
    "\n",
    "Model bag-of-words, atau disingkat BoW, adalah cara mengekstrak fitur dari teks untuk digunakan dalam pemodelan, seperti dengan algoritme machine learning.\n",
    "\n",
    "Pendekatannya sangat sederhana dan fleksibel, dan dapat digunakan dalam berbagai cara untuk mengekstrak fitur dari dokumen.\n",
    "\n",
    "bag of words (**kantong kata**) adalah representasi teks yang menggambarkan kemunculan kata-kata dalam dokumen. Ini melibatkan dua hal:\n",
    "\n",
    "Kosakata dari kata-kata yang dikenal.\n",
    "Ukuran keberadaan kata-kata yang dikenal.\n",
    "Ini disebut “kantong” kata, karena informasi tentang urutan atau struktur kata dalam dokumen akan dibuang. Model hanya memperhatikan apakah kata-kata yang diketahui muncul dalam dokumen, bukan di mana dalam dokumen.\n",
    "\n",
    "`Prosedur ekstraksi fitur yang sangat umum untuk kalimat dan dokumen adalah pendekatan bag-of-words (BOW). Dalam pendekatan ini, kami melihat histogram kata-kata di dalam teks, yaitu mempertimbangkan setiap jumlah kata sebagai fitur.`\n",
    "\n",
    "Intuisinya adalah bahwa dokumen itu serupa jika memiliki konten yang serupa. Lebih lanjut, dari isinya saja kita dapat mempelajari sesuatu tentang arti dari dokumen tersebut.\n",
    "\n",
    "Kantong kata bisa sesederhana atau serumit yang Anda suka. Kompleksitas muncul baik dalam memutuskan bagaimana merancang kosakata dari kata-kata yang dikenal (atau token) dan bagaimana menilai keberadaan kata-kata yang dikenal.\n",
    "\n",
    "Kami akan melihat lebih dekat pada kedua masalah ini.\n",
    "\n",
    "## Tahapan BOW\n",
    "### Step 1: Collect data\n",
    "```\n",
    "Itu adalah saat terbaik,\n",
    "itu adalah saat terburuk,\n",
    "itu adalah zaman kebijaksanaan,\n",
    "itu adalah zaman kebodohan,\n",
    "```\n",
    "### Step 2: Design Vocabulary\n",
    "Sekarang kita bisa membuat daftar semua kata dalam model kosakata kita.\n",
    "\n",
    "Kata-kata unik di sini (mengabaikan huruf besar dan tanda baca) adalah:\n",
    "\n",
    "```\n",
    "- itu\n",
    "- adalah\n",
    "- saat\n",
    "- terbaik\n",
    "- terburuk\n",
    "- zaman\n",
    "- kebijaksanaan\n",
    "- kebodohan\n",
    "```\n",
    "\n",
    "kosakata terdiri dari 8 huruf dari corpus yang berisi 16 kata\n",
    "\n",
    "### Step 3: membuat dokumen vector\n",
    "Langkah selanjutnya adalah menilai kata-kata di setiap dokumen.\n",
    "\n",
    "Tujuannya adalah untuk mengubah setiap dokumen teks bebas menjadi vektor yang dapat kita gunakan sebagai masukan atau keluaran untuk model pembelajaran mesin.\n",
    "\n",
    "Karena kita mengetahui kosakata memiliki 8 kata, kita dapat menggunakan representasi dokumen panjang tetap 8, dengan satu posisi dalam vektor untuk menilai setiap kata.\n",
    "\n",
    "Metode penilaian yang paling sederhana adalah menandai keberadaan kata sebagai nilai boolean, 0 untuk tidak ada, 1 untuk saat ini.\n",
    "\n",
    "Dengan menggunakan urutan sembarang kata yang tercantum di atas dalam kosakata kita, kita dapat melangkah melalui dokumen pertama (\"**Itu adalah saat terbaik**\") dan mengubahnya menjadi vektor biner.\n",
    "\n",
    "Penilaian dokumen akan terlihat sebagai berikut:\n",
    "\n",
    "| kata           | binary vector|\n",
    "|----------------|--------------|\n",
    "| itu            | 1            |\n",
    "| adalah         | 1            |\n",
    "| saat           | 1            |\n",
    "| terbaik        | 1            |\n",
    "| terburuk       | 0            |\n",
    "| zaman          | 0            |\n",
    "| kebijaksanaan  | 0            |\n",
    "| kebodohan      | 0            |\n",
    "\n",
    "dan binary vectornya adalah sebagai berikut:\n",
    "```\n",
    "    \"itu adalah saat terbaik\" = [1,1,1,1,0,0,0,0]\n",
    "```\n",
    "\n",
    "lalu kata lain akan menjadi sebagai berikut:\n",
    "```\n",
    "    itu adalah saat terburuk       =  [1,1,1,0,1,0,0,0]\n",
    "    itu adalah zaman kebijaksanaan =  [1,1,0,0,0,1,0]\n",
    "    itu adalah zaman kebodohan     =  [1,1,0,0,0,1,0,1]\n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**kasus**:\n",
    "\n",
    "|kata                    |itu |adalah |saat |terbaik |terburuk |zaman |kebijaksanaan | kebodohan|\n",
    "|------------------------|:--:|:-----:|:---:|:------:|:-------:|:----:|:------------:|:--------:|\n",
    "|itu adalah saat terbaik | 1  | 1     | 1   | 1      | 0       | 0.   | 0            | 0        |\n",
    "\n",
    "Semua urutan kata dibuang secara nominal dan kami memiliki cara yang konsisten untuk mengekstraksi fitur dari dokumen mana pun di korpus kami, siap digunakan dalam pemodelan.\n",
    "\n",
    "Dokumen baru yang tumpang tindih dengan kosakata dari kata-kata yang dikenal, tetapi mungkin berisi kata-kata di luar kosakata, masih dapat dikodekan, di mana hanya kemunculan kata-kata yang diketahui diberi skor dan kata-kata yang tidak dikenal diabaikan.\n",
    "\n",
    "Anda dapat melihat bagaimana ini secara alami dapat diskalakan ke kosakata besar dan dokumen lebih besar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengatur Kosakata\n",
    "Ketika ukuran kosakata meningkat, begitu pula representasi vektor dokumen.\n",
    "\n",
    "Pada contoh sebelumnya, panjang vektor dokumen sama dengan banyaknya kata yang diketahui.\n",
    "\n",
    "Anda dapat membayangkan bahwa untuk korpus yang sangat besar, seperti ribuan buku, panjang vektor mungkin ribuan atau jutaan posisi. Lebih lanjut, setiap dokumen mungkin berisi sangat sedikit kata yang diketahui dalam kosakata.\n",
    "\n",
    "Ini menghasilkan vektor dengan banyak skor nol, yang disebut vektor renggang atau representasi renggang.\n",
    "\n",
    "Vektor renggang membutuhkan lebih banyak memori dan sumber daya komputasi saat pemodelan dan sejumlah besar posisi atau dimensi dapat membuat proses pemodelan sangat menantang untuk algoritme tradisional.\n",
    "\n",
    "Dengan demikian, ada tekanan untuk mengurangi ukuran kosakata saat menggunakan model bag-of-words.\n",
    "\n",
    "Ada teknik pembersihan teks sederhana yang dapat digunakan sebagai langkah pertama, seperti:\n",
    "\n",
    "- Mengabaikan Besar Kecil huruf\n",
    "- Mengabaikan tanda baca\n",
    "- Mengabaikan kata-kata umum yang tidak mengandung banyak informasi, yang disebut kata-kata henti, seperti \"yang\" \"dari,\" dll. **stop words**\n",
    "- Memperbaiki kata yang salah eja.\n",
    "- Mengurangi kata-kata ke akarnya (misalnya \"main\" dari \"bermain\") menggunakan algoritme stemming. **sastrawi**\n",
    "\n",
    "Pendekatan yang lebih canggih adalah dengan membuat kosakata dari kata-kata yang dikelompokkan. Ini mengubah cakupan kosakata dan memungkinkan kumpulan kata-kata menangkap sedikit lebih banyak makna dari dokumen.\n",
    "\n",
    "Dalam pendekatan ini, setiap kata atau token disebut “gram”. Menciptakan kosakata pasangan dua kata, pada gilirannya, disebut model bigram. Sekali lagi, hanya bigram yang muncul di korpus yang dimodelkan, tidak semua bigram yang memungkinkan.\n",
    "\n",
    "```\n",
    "N-gram adalah urutan N-token dari kata-kata: \n",
    "misal ada kata: **segera berkumpul disini sekarang**, maka\n",
    "- 2-gram (lebih sering disebut bigram) adalah urutan dua kata dari kata-kata seperti \"segera berkumpul\" dan \"berkumpul disini\" dan \"disini sekarang\"\n",
    "- 3-gram (lebih sering disebut trigram) adalah urutan tiga kata dari kata-kata seperti: \"segera berkumpul disini\" dan \"berkumpul disini sekarang\"\n",
    "```\n",
    "\n",
    "kembali ke contoh awal: `itu adalah saat terbaik` dapat dipecah bigram menjadi:\n",
    "- itu adalah\n",
    "- adalah saat\n",
    "- saat terbaik\n",
    "\n",
    "```\n",
    "representasi bag-of-bigram jauh lebih kuat daripada kantong-kata-kata, dan dalam banyak kasus terbukti sangat sulit untuk dikalahkan.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Words\n",
    "Setelah kosakata dipilih, kemunculan kata-kata dalam dokumen contoh perlu dinilai.\n",
    "\n",
    "Dalam contoh yang berhasil, kita telah melihat satu pendekatan yang sangat sederhana untuk menilai: penilaian biner dari ada atau tidaknya kata.\n",
    "\n",
    "Beberapa metode penilaian sederhana tambahan meliputi:\n",
    "\n",
    "- **counts** (Hitungan). Hitung berapa kali setiap kata muncul dalam dokumen.\n",
    "- **Frekuensi** Hitung frekuensi setiap kata muncul di dokumen dari semua kata di dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing Kata\n",
    "Anda mungkin ingat dari ilmu komputer bahwa fungsi hash adalah sedikit matematika yang memetakan data ke kumpulan angka berukuran tetap.\n",
    "\n",
    "Misalnya, kami menggunakannya dalam tabel hash saat pemrograman di mana mungkin nama diubah menjadi angka untuk pencarian cepat.\n",
    "\n",
    "Kita dapat menggunakan representasi hash dari kata-kata yang dikenal dalam kosakata kita. Ini mengatasi masalah memiliki kosakata yang sangat besar untuk korpus teks yang besar karena kita dapat memilih ukuran ruang hash, yang pada gilirannya adalah ukuran representasi vektor dari dokumen.\n",
    "\n",
    "Kata-kata di-hash secara deterministik ke indeks integer yang sama dalam ruang hash target. Skor atau hitungan biner kemudian dapat digunakan untuk menilai kata.\n",
    "\n",
    "Ini disebut \"trik hash\" atau \"hashing fitur\".\n",
    "\n",
    "Tantangannya adalah memilih ruang hash untuk mengakomodasi ukuran kosa kata yang dipilih guna meminimalkan kemungkinan tabrakan dan ketersebaran trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kelemahan BOW\n",
    "\n",
    "Model bag-of-words sangat mudah dipahami dan diterapkan, serta menawarkan banyak fleksibilitas untuk penyesuaian pada data teks spesifik Anda.\n",
    "\n",
    "Ini telah digunakan dengan sukses besar pada masalah prediksi seperti pemodelan bahasa dan klasifikasi dokumentasi.\n",
    "\n",
    "Namun demikian, ada beberapa kekurangan, seperti:\n",
    "\n",
    "- Kosakata: Kosakata memerlukan desain yang cermat, terutama untuk mengelola ukuran, yang memengaruhi ketersebaran representasi dokumen.\n",
    "- Ketersebaran: Representasi renggang lebih sulit untuk dimodelkan baik karena alasan komputasi (kompleksitas ruang dan waktu) dan juga karena alasan informasi, di mana tantangannya adalah bagi model untuk memanfaatkan begitu sedikit informasi dalam ruang representasi yang begitu besar.\n",
    "- Arti: Membuang urutan kata mengabaikan konteks, dan pada gilirannya berarti kata-kata dalam dokumen (semantik). Konteks dan makna dapat menawarkan banyak hal kepada model, yang jika dimodelkan dapat membedakan antara kata-kata yang sama yang disusun secara berbeda (\"ini menarik\" vs \"apakah ini menarik\"), sinonim (\"sepeda tua\" vs \"sepeda bekas\") , dan banyak lagi.\n",
    "- Jika kalimat baru mengandung kata-kata baru, maka ukuran kosakata kita akan bertambah dan dengan demikian, panjang vektor juga akan bertambah.\n",
    "- Selain itu, vektor juga akan berisi banyak 0, sehingga menghasilkan matriks renggang (yang ingin kita hindari)\n",
    "- Kami tidak menyimpan informasi tentang tata bahasa kalimat atau urutan kata-kata dalam teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF merupakan singkatan dari Term Frequency — Inverse Document Frequency. Sejatinya, TF-IDF merupakan gabungan dari 2 proses yaitu Term Frequency (TF) dan Inverse Document Frequency (IDF).\n",
    "\n",
    "TF-IDF biasa digunakan ketika kita ingin mengubah data teks menjadi vektor namun dengan memperhatikan apakah sebuah kata tersebut cukup informatif atau tidak. Mudahnya, TF-IDF membuat kata yang sering muncul memiliki nilai yang cenderung kecil, sedangkan untuk kata yang semakin jarang muncul akan memiliki nilai yang cenderung besar. Kata yang sering muncul disebut juga Stopwords biasanya dianggap kurang penting, salah satu contohnya adalah kata hubung (yang, di, akan, dengan, dll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF)\n",
    "\n",
    "Term Frequency (TF) menghitung frekuensi jumlah kemunculan kata pada sebuah dokumen. Karena panjang dari setiap dokumen bisa berbeda-beda, maka umumnya nilai TF ini dibagi dengan panjang dokumen (jumlah seluruh kata pada dokumen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Document Frequency (IDF)\n",
    "\n",
    "Setelah berhasil menghitung nilai Term Frequency, selanjutnya hitung nilai Inverse Document Frequency (IDF), yang merupakan nilai untuk mengukur seberapa penting sebuah kata. IDF akan menilai kata yang sering muncul sebagai kata yang kurang penting berdasarkan kemunculan kata tersebut pada seluruh dokumen. Semakin kecil nilai IDF maka akan dianggap semakin tidak penting kata tersebut, begitu pula sebaliknya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kekurangan TF-IDF\n",
    "\n",
    "TF-IDF sejatinya berdasar pada Bag of Words (BoW), sehingga TF-IDF pun tidak bisa menangkap posisi teks dan semantiknya.\n",
    "TF-IDF hanya berguna sebagai fitur di level leksikal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasifikasi Dokumen\n",
    "\n",
    "![klasifikasi](gb1.png)\n",
    "\n",
    "Klasifikasi teks adalah proses pemberian tag atau kategori ke teks menurut isinya. Klasifikasi teks dapat digunakan untuk mengatur, menyusun, dan mengkategorikan hampir semua hal. Misalnya, artikel baru dapat diatur berdasarkan topik, percakapan obrolan dapat diatur berdasarkan bahasa, penyebutan merek dapat diatur berdasarkan sentimen, dan sebagainya.\n",
    "\n",
    "### Bagaimana Klasifikasi Teks Bekerja?\n",
    "\n",
    "Klasifikasi teks dapat dilakukan dengan dua cara berbeda: klasifikasi manual dan otomatis. Cara yang pertama, annotator manusia menafsirkan konten teks dan mengkategorikannya. Metode ini biasanya dapat memberikan hasil yang berkualitas tetapi memakan waktu dan mahal. Yang terakhir menerapkan pembelajaran mesin, pemrosesan bahasa alami, dan teknik lainnya untuk secara otomatis mengklasifikasikan teks dengan cara yang lebih cepat dan lebih hemat biaya. Ada banyak pendekatan untuk klasifikasi teks otomatis, yang dapat dikelompokkan menjadi tiga jenis sistem yang berbeda :\n",
    "\n",
    "- Rule-based systems\n",
    "- Machine Learning based systems\n",
    "- Hybrid systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based systems\n",
    "\n",
    "Pendekatan berbasis aturan mengklasifikasikan teks ke dalam kelompok terorganisir dengan menggunakan seperangkat aturan linguistik buatan tangan. Aturan-aturan ini menginstruksikan sistem untuk menggunakan elemen teks yang relevan secara semantik untuk mengidentifikasi kategori yang relevan berdasarkan isinya. Setiap aturan terdiri dari anteseden atau pola dan kategori yang diprediksi. \n",
    "\n",
    "Misalnya ingin mengelompokkan artikel berita menjadi 2 kelompok, yaitu Olahraga dan Politik. \n",
    "\n",
    "- Pertama, Perlu menentukan dua daftar kata yang menjadi ciri masing-masing kelompok (misalnya kata-kata yang berkaitan dengan olahraga seperti sepak bola, bola basket, LeBron James, dll dan kata-kata yang berkaitan dengan politik seperti Donald Trump, Hillary Clinton, Putin, dll. \n",
    "- Selanjutnya, saat ingin mengklasifikasikan teks baru yang masuk, maka perlu menghitung jumlah kata terkait olahraga yang muncul di teks dan melakukan hal yang sama untuk kata terkait politik. Jika jumlah kemunculan kata terkait olahraga lebih banyak daripada jumlah kata terkait politik, maka teks tersebut diklasifikasikan sebagai olahraga dan sebaliknya.\n",
    "\n",
    "Misalnya, sistem berbasis aturan ini akan mengklasifikasikan judul “Kapan pertandingan pertama LeBron James dengan Lakers?” sebagai Olahraga karena menghitung 1 istilah terkait olahraga (Lebron James) dan tidak menghitung istilah terkait politik apa pun.\n",
    "\n",
    "Sistem berbasis aturan dapat dipahami manusia dan dapat ditingkatkan seiring waktu. Tetapi pendekatan ini memiliki beberapa kelemahan. Sebagai permulaan, sistem ini membutuhkan pengetahuan yang mendalam tentang domain. Mereka juga memakan waktu, karena membuat aturan untuk sistem yang kompleks bisa sangat menantang dan biasanya membutuhkan banyak analisis dan pengujian. Sistem berbasis aturan juga sulit dipertahankan dan tidak diskalakan dengan baik karena menambahkan aturan baru dapat mempengaruhi hasil aturan yang sudah ada sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning based systems\n",
    "\n",
    "Klasifikasi teks dengan machine learning membuat klasifikasi berdasarkan pengamatan sebelumnya. Dengan menggunakan contoh yang diberi label sebelumnya sebagai data pelatihan, algoritma machine learning dapat mempelajari asosiasi yang berbeda antara bagian teks dan bahwa keluaran tertentu (yaitu tag) diharapkan untuk masukan tertentu (yaitu teks).\n",
    "\n",
    "Langkah pertama untuk melatih pengklasifikasi dengan pembelajaran mesin adalah ekstraksi fitur: metode yang digunakan untuk mengubah setiap teks menjadi representasi numerik dalam bentuk vektor. Salah satu pendekatan yang paling sering digunakan adalah bag of words, di mana vektor mewakili frekuensi suatu kata dalam kamus kata yang telah ditentukan sebelumnya.\n",
    "\n",
    "Misalnya, jika kita telah mendefinisikan kamus kita untuk memiliki kata-kata berikut {This, is, the, not, awesome, bad, basketball}, dan kita ingin membuat vektor teks “This is awesome”, kita akan memiliki representasi vektor berikut dari teks itu: (1, 1, 0, 0, 1, 0, 0). \n",
    "\n",
    "Kemudian, algoritma machine learning diisi dengan data pelatihan yang terdiri dari pasangan set fitur (vektor untuk setiap contoh teks) dan tag (misalnya olahraga, politik) untuk menghasilkan model klasifikasi.\n",
    "\n",
    "Setelah dilatih dengan sampel pelatihan yang cukup, model machine learning dapat mulai membuat prediksi yang akurat. Ekstraktor fitur yang sama digunakan untuk mengubah teks yang tidak terlihat menjadi kumpulan fitur yang dapat dimasukkan ke dalam model klasifikasi untuk mendapatkan prediksi pada tag (misalnya olahraga, politik).\n",
    "\n",
    "Klasifikasi teks dengan machine learning biasanya jauh lebih akurat daripada sistem aturan buatan manusia, terutama pada tugas klasifikasi yang kompleks. Selain itu, pengklasifikasi dengan machine learning lebih mudah dikelola dan Anda selalu dapat memberi tag pada contoh baru untuk mempelajari tugas baru.\n",
    "\n",
    "Beberapa algoritma machine learning yang paling populer untuk membuat model klasifikasi teks termasuk kelompok algoritma naive bayes family of algorithms, support vector machines, dan deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid systems\n",
    "\n",
    "Sistem hybrid menggabungkan pengklasifikasi dasar yang dilatih dengan machine learning dan rule-based system, yang digunakan untuk lebih meningkatkan hasil. Sistem hybrid ini dapat dengan mudah disesuaikan dengan menambahkan aturan khusus untuk tag yang bentrok yang belum dimodelkan dengan benar oleh pengklasifikasi dasar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and Evaluation\n",
    "\n",
    "Validasi silang adalah metode umum untuk mengevaluasi kinerja pengklasifikasi teks. Ini terdiri dari pemisahan kumpulan data pelatihan secara acak menjadi kumpulan contoh yang panjangnya sama (misalnya 4 kumpulan dengan 25% data). Untuk setiap set, pengklasifikasi teks dilatih dengan sampel yang tersisa (misalnya 75% dari sampel). Selanjutnya, pengklasifikasi membuat prediksi pada set masing-masing dan hasilnya dibandingkan dengan tag yang dianotasi manusia. Hal ini memungkinkan untuk menemukan kapan prediksi benar (positif benar dan negatif benar) dan ketika membuat kesalahan (positif palsu, negatif palsu).\n",
    "\n",
    "Dengan hasil ini,akan dapat membuat metrik performa yang berguna untuk penilaian cepat tentang seberapa baik pengklasifikasi bekerja :\n",
    "- <b>Accuracy</b> : persentase teks yang diprediksi dengan tag yang benar.\n",
    "- <b>Precision</b> : persentase contoh yang didapatkan pengklasifikasi dari jumlah total contoh yang diprediksi untuk tag tertentu.\n",
    "- <b>Recall</b> : persentase contoh yang diprediksi oleh pengklasifikasi untuk tag tertentu dari jumlah total contoh yang seharusnya diprediksi untuk tag tersebut.\n",
    "- <b>F1 Score</b>: rata-rata presisi dan perolehan yang harmonis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
